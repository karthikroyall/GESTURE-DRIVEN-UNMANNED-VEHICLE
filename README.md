The advancement of human-machine interaction has led to the development of intuitive and contactless control mechanisms for robotic systems. One such approach is gesture-based control, which enables seamless communication between humans and machines without the need for physical contact. This project focuses on the development of a gesture-driven unmanned vehicle that utilizes computer vision, machine learning, and wireless communication to interpret human hand gestures and translate them into vehicle movement commands. The primary objective is to design an efficient and user-friendly interface for controlling a remote vehicle, with potential applications in automation, assistive technology, remote operations, and defense.
The proposed system comprises three main components. First, the Gesture Recognition Module involves a laptop camera capturing real-time hand gestures. A trained machine learning model classifies the gestures into predefined commands such as forward, backward, left, right, and stop. The processed data is then converted into control signals.
Second, the Wireless Communication Interface ensures that the recognized gesture command is transmitted wirelessly using an NRF24L01 transceiver module connected to an Arduino UNO R3. This enables real-time communication between the laptop and the unmanned vehicle.
Third, the Vehicle Control and Execution component consists of another NRF24L01 module, connected to an Arduino UNO R3, receiving the command. The Arduino processes the signal and controls the motors using TB6612FNG motor drivers. The vehicle, powered by 18650 Li-ion batteries, moves according to the received instructions.
The system relies on various hardware components to ensure smooth functionality. These include a Laptop for gesture recognition and processing, Arduino UNO R3 for signal transmission and vehicle control, NRF24L01 for wireless communication, BO motors for vehicle movement, TB6612FNG motor drivers for additional motor control support, and 18650 Li-ion batteries as the power source for the vehicle.
The system follows a structured workflow to ensure accurate and efficient gesture-based vehicle control. First, the Gesture Detection and Processing stage involves capturing the user’s hand gestures using the laptop’s camera. These images are processed in real time using OpenCV and a trained deep learning model that classifies different gestures.
Next, in the Wireless Signal Transmission stage, the classified gesture is converted into a digital signal and transmitted wirelessly to the unmanned vehicle using an NRF24L01 module connected to the laptop’s Arduino. Finally, the Vehicle Movement Execution stage ensures that upon receiving the signal, the Arduino on the vehicle interprets the command and drives the BO motors accordingly using the motor driver shield.
This project has numerous practical applications. In Assistive Technology, it enables individuals with mobility impairments to control robotic wheelchairs or other assistive devices using hand gestures. For Remote Vehicle Control, it proves useful for industries requiring contactless operation of robotic vehicles in hazardous environments. In Automation and Robotics, it enhances automation systems by enabling intuitive human-machine interaction. Additionally, in Defense and Military Applications, gesture-controlled unmanned ground vehicles (UGVs) can be deployed in surveillance and reconnaissance missions.
The project is designed to be modular and scalable, allowing for future advancements. Integration of Obstacle Avoidance Sensors using ultrasonic or LiDAR sensors can prevent collisions. Autonomous Navigation can be implemented by using AI-based decision-making for self- navigation. IoT Connectivity can be introduced to enable remote monitoring and control via mobile applications or cloud-based systems. Additionally, an Extended Gesture Library can expand the system to recognize more complex gestures for advanced functionalities.
This gesture-driven unmanned vehicle project successfully integrates computer vision, embedded systems, and wireless communication to enable intuitive vehicle control. By leveraging machine learning for gesture recognition and embedded systems for real-time execution, this system demonstrates an innovative approach to human-machine interaction. The proposed solution has vast applications in automation, assistive technology, remote operations, and defense, making it a valuable contribution to the field of embedded robotics. Future enhancements will focus on improving efficiency, expanding functionality, and integrating additional smart features to enhance its usability and adaptability across various domains.
